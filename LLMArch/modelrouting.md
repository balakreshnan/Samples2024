# Pick the best model for inference per use case

## Introduction

- Idea here is to pick the best model for inference per use task.
- We have multiple models for each use case.
- It could be picking Large language model or small language models
- or could be vendor specific models or open source models
- or could be custom models

## design process

![info](https://github.com/balakreshnan/Samples2024/blob/main/LLMArch/images/modelroutingarch1.jpg 'RagChat')

- Flow show how to pick the best model for inference
- based on task and available models in various vendors
- Could be meta data driven based selection
- Could be based on performance